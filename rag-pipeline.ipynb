{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea99448-3de3-4500-833f-d2f2f9dafd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushm\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# 1. Imports & Environment\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078ff43d-3c84-4fa7-a555-f2a11e943265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Using Ollama model: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# 2. Configure Model Backend (Ollama / OpenAI)\n",
    "# ==============================================\n",
    "\n",
    "MODEL_BACKEND = os.getenv(\"MODEL_BACKEND\", \"ollama\")   # \"ollama\" or \"openai\"\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"llama3.2\")\n",
    "OLLAMA_API_BASE = os.getenv(\"OLLAMA_API_BASE\", \"http://localhost:11434\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"ollama\")  # dummy for ollama\n",
    "\n",
    "if MODEL_BACKEND.lower() == \"ollama\":\n",
    "    os.environ[\"OPENAI_API_BASE\"] = f\"{OLLAMA_API_BASE}/v1\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    print(f\"ðŸ”— Using Ollama model: {MODEL_NAME}\")\n",
    "else:\n",
    "    print(f\"ðŸ”— Using cloud backend: {MODEL_NAME}\")\n",
    "\n",
    "client_ai = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6ca8253-009a-4bf4-88cc-06abe27343aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Existing Collections:\n",
      " - DBMS-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushm\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sushm\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\sushm\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\sushm\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# 3. Load ChromaDB & Embedding Model\n",
    "# ======================================\n",
    "\n",
    "CHROMA_PATH = \"./chroma_store_768\"\n",
    "\n",
    "client_chroma = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "print(\"ðŸ“¦ Existing Collections:\")\n",
    "for c in client_chroma.list_collections():\n",
    "    print(\" -\", c.name)\n",
    "\n",
    "# load correct collection\n",
    "collection = client_chroma.get_collection(\"DBMS-25\")\n",
    "\n",
    "# embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bfcc5fc-3789-4fcb-8057-7f1f06b2ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 4. User Query\n",
    "# ==========================\n",
    "\n",
    "query = \"instagram reach aalysis\"\n",
    "TOP_K = 10\n",
    "\n",
    "query_emb = embedder.encode(query).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbadd123-0a2c-4e61-a089-ab39b75fd8eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: auto001\n",
      "Add of existing embedding ID: auto002\n",
      "Add of existing embedding ID: auto003\n",
      "Add of existing embedding ID: auto004\n",
      "Add of existing embedding ID: auto005\n",
      "Add of existing embedding ID: auto006\n",
      "Add of existing embedding ID: auto007\n",
      "Add of existing embedding ID: auto008\n",
      "Add of existing embedding ID: auto009\n",
      "Add of existing embedding ID: auto010\n",
      "Add of existing embedding ID: auto011\n",
      "Add of existing embedding ID: auto012\n",
      "Add of existing embedding ID: auto013\n",
      "Add of existing embedding ID: auto014\n",
      "Add of existing embedding ID: auto015\n",
      "Add of existing embedding ID: auto001\n",
      "Add of existing embedding ID: auto002\n",
      "Add of existing embedding ID: auto003\n",
      "Add of existing embedding ID: auto004\n",
      "Add of existing embedding ID: auto005\n",
      "Add of existing embedding ID: auto006\n",
      "Add of existing embedding ID: auto007\n",
      "Add of existing embedding ID: auto008\n",
      "Add of existing embedding ID: auto009\n",
      "Add of existing embedding ID: auto010\n",
      "Add of existing embedding ID: auto001\n",
      "Add of existing embedding ID: auto002\n",
      "Add of existing embedding ID: auto003\n",
      "Add of existing embedding ID: auto004\n",
      "Add of existing embedding ID: auto005\n",
      "Add of existing embedding ID: auto006\n",
      "Add of existing embedding ID: auto007\n",
      "Add of existing embedding ID: auto008\n",
      "Add of existing embedding ID: auto009\n",
      "Add of existing embedding ID: auto010\n",
      "Add of existing embedding ID: auto011\n",
      "Add of existing embedding ID: auto012\n",
      "Add of existing embedding ID: auto013\n",
      "Add of existing embedding ID: auto014\n",
      "Add of existing embedding ID: auto015\n",
      "Add of existing embedding ID: auto016\n",
      "Add of existing embedding ID: auto017\n",
      "Add of existing embedding ID: auto018\n",
      "Add of existing embedding ID: auto019\n",
      "Add of existing embedding ID: auto020\n",
      "Add of existing embedding ID: auto021\n",
      "Add of existing embedding ID: auto022\n",
      "Add of existing embedding ID: auto023\n",
      "Add of existing embedding ID: auto024\n",
      "Add of existing embedding ID: auto025\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# 5. Query ChromaDB for Similar Docs\n",
    "# ===================================\n",
    "\n",
    "res = collection.query(\n",
    "    query_embeddings=[query_emb],\n",
    "    n_results=TOP_K,\n",
    "    include=[\"metadatas\", \"documents\", \"distances\", \"embeddings\"]\n",
    ")\n",
    "\n",
    "ids       = res[\"ids\"][0]\n",
    "docs      = res[\"documents\"][0]\n",
    "metas     = res[\"metadatas\"][0]\n",
    "distances = res[\"distances\"][0]\n",
    "stored_embs = res[\"embeddings\"][0]\n",
    "\n",
    "def to_percent(x): return round(float(x) * 100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2acc83-fe72-4da5-bfcb-d9b206d0cf1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6. Compute Similarities with Sentence Transformers\n",
    "# ========================================\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, rid in enumerate(ids):\n",
    "\n",
    "    meta = metas[i] or {}\n",
    "    title = meta.get(\"title\", \"\")\n",
    "    domain = meta.get(\"domain\", \"\")\n",
    "    tech_stack = meta.get(\"tech_stack\", \"\")\n",
    "    source = meta.get(\"source\", \"\")\n",
    "    desc = docs[i] or \"\"\n",
    "    objective = meta.get(\"objective\", \"\")\n",
    "\n",
    "    # embeddings\n",
    "    def enc(x): return embedder.encode(x).tolist() if x else None\n",
    "\n",
    "    emb_title = enc(title)\n",
    "    emb_desc = enc(desc)\n",
    "    emb_tech = enc(tech_stack)\n",
    "    emb_obj  = enc(objective)\n",
    "\n",
    "    # similarity metrics\n",
    "    sim_whole = cosine_similarity([query_emb], [stored_embs[i]])[0][0]\n",
    "    sim_title = cosine_similarity([query_emb], [emb_title])[0][0] if emb_title else 0\n",
    "    sim_desc  = cosine_similarity([query_emb], [emb_desc])[0][0] if emb_desc else 0\n",
    "    sim_tech  = cosine_similarity([query_emb], [emb_tech])[0][0] if emb_tech else 0\n",
    "    sim_obj   = cosine_similarity([query_emb], [emb_obj])[0][0] if emb_obj else 0\n",
    "\n",
    "    results.append({\n",
    "        \"id\": rid,\n",
    "        \"title\": title,\n",
    "        \"domain\": domain,\n",
    "        \"tech_stack\": tech_stack,\n",
    "        \"source\": source,\n",
    "        \"sim_whole\": sim_whole,\n",
    "        \"sim_title\": sim_title,\n",
    "        \"sim_description\": sim_desc,\n",
    "        \"sim_tech_stack\": sim_tech,\n",
    "        \"sim_objective\": sim_obj,\n",
    "        \"doc_snippet\": desc[:300]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91588ac0-6391-45b2-be40-221b06c640b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ðŸŽ¯ Final Unique Top-5 ===\n",
      "\n",
      "1. Rainfall Prediction system (27.09%)\n",
      "2. To analyse the implications of deploying autonomous drones (25.02%)\n",
      "3. Weather Forecasting Application (22.7%)\n",
      "4. Predictive Analysis Tool (22.22%)\n",
      "5. Medical insurance price prediction (22.06%)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 7. Deduplicate by Title â†’ Keep Top 5\n",
    "# =========================================\n",
    "\n",
    "unique = {}\n",
    "for r in sorted(results, key=lambda x: x[\"sim_whole\"], reverse=True):\n",
    "    if r[\"title\"] not in unique:\n",
    "        unique[r[\"title\"]] = r\n",
    "\n",
    "final_results = list(unique.values())[:5]\n",
    "\n",
    "print(\"\\n=== ðŸŽ¯ Final Unique Top-5 ===\\n\")\n",
    "for i, r in enumerate(final_results, start=1):\n",
    "    print(f\"{i}. {r['title']} ({to_percent(r['sim_whole'])}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa7443d-87ca-417e-8ddf-4b372ebca5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 8. Build Enhanced Prompt for Llama3.2\n",
    "# =============================================\n",
    "\n",
    "prompt_lines = []\n",
    "\n",
    "prompt_lines.append(f\"Query: {query}\")\n",
    "prompt_lines.append(\"The system retrieved the 5 most similar projects.\\n\")\n",
    "\n",
    "for idx, r in enumerate(final_results, start=1):\n",
    "    prompt_lines.append(f\"{idx}. {r['title']}\")\n",
    "    prompt_lines.append(f\"   - Domain: {r['domain']}\")\n",
    "    prompt_lines.append(\n",
    "        f\"   - Similarities: Overall {to_percent(r['sim_whole'])}%, \"\n",
    "        f\"Title {to_percent(r['sim_title'])}%, Description {to_percent(r['sim_description'])}%, Tech {to_percent(r['sim_tech_stack'])}%\"\n",
    "    )\n",
    "    prompt_lines.append(f\"   - Snippet: {r['doc_snippet']}\\n\")\n",
    "\n",
    "prompt_lines.append(\n",
    "\"\"\"\n",
    "Act as a project evaluation assistant.\n",
    "\n",
    "For **each** project:\n",
    "1. Explain briefly why it matched.\n",
    "2. Identify exact overlapping parts.\n",
    "3. Provide 2 suggestions to make the project idea more original.\n",
    "\n",
    "Finally:\n",
    "Give 3 general originality improvement tips.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llama_prompt = \"\\n\".join(prompt_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6a57d46-8058-4efe-b2c7-991fa77183ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Generating Llama3.2 similarity analysis...\n",
      "\n",
      "I'll evaluate each project and provide feedback.\n",
      "\n",
      "**Project 1: Scraping laptop data from Amazon**\n",
      "\n",
      "1. Why it matched: The project's title, description, and tech domain are similar to the query \"web scrapping amazon deals\", indicating that it involves extracting data from a website using web scraping techniques.\n",
      "2. Exact overlapping parts: None found.\n",
      "3. Suggestions for originality improvement:\n",
      "\t* Add a twist by focusing on a specific type of laptop data (e.g., gaming laptops, budget laptops) or exploring the use of machine learning algorithms to improve data extraction accuracy.\n",
      "\t* Incorporate a unique aspect, such as using natural language processing techniques to extract reviews or ratings from Amazon product pages.\n",
      "\n",
      "**Project 2: Extracting Laptop Data from Amazon**\n",
      "\n",
      "1. Why it matched: Similar to Project 1, this project's title and description are similar to the query \"web scrapping amazon deals\".\n",
      "2. Exact overlapping parts: None found.\n",
      "3. Suggestions for originality improvement:\n",
      "\t* Emphasize a specific aspect of laptop data extraction, such as handling pagination or dealing with anti-scraping measures on Amazon.\n",
      "\t* Explore using web scraping frameworks like Scrapy or Selenium to improve efficiency and scalability.\n",
      "\n",
      "**Project 3: Beautiful Soup for Web Scraping**\n",
      "\n",
      "1. Why it matched: Although not directly related to Amazon deals, this project's focus on web scraping techniques is relevant to the query.\n",
      "2. Exact overlapping parts: None found.\n",
      "3. Suggestions for originality improvement:\n",
      "\t* Focus on a specific use case for Beautiful Soup, such as extracting data from e-commerce websites or handling complex HTML structures.\n",
      "\t* Incorporate a unique aspect, such as using machine learning algorithms to improve web scraping efficiency or accuracy.\n",
      "\n",
      "**Project 4: Online Payment Fraud Detection**\n",
      "\n",
      "1. Why it matched: Although not directly related to Amazon deals, this project's focus on AI and data science is relevant to the query.\n",
      "2. Exact overlapping parts: None found.\n",
      "3. Suggestions for originality improvement:\n",
      "\t* Emphasize a specific aspect of online payment fraud detection, such as handling different types of transactions or identifying high-risk users.\n",
      "\t* Explore using deep learning techniques, such as neural networks or convolutional neural networks, to improve fraud detection accuracy.\n",
      "\n",
      "**Project 5: Automated Detection of Online Payment Frauds**\n",
      "\n",
      "1. Why it matched: Similar to Project 4, this project's focus on AI and data science is relevant to the query.\n",
      "2. Exact overlapping parts: None found.\n",
      "3. Suggestions for originality improvement:\n",
      "\t* Focus on a specific use case for automated payment fraud detection, such as handling transactions in multiple currencies or identifying fraudulent activity on mobile devices.\n",
      "\t* Incorporate a unique aspect, such as using natural language processing techniques to analyze transaction descriptions and identify potential red flags.\n",
      "\n",
      "**General Originality Improvement Tips:**\n",
      "\n",
      "1. **Combine multiple domains**: Instead of focusing on a single domain (e.g., web scraping), combine it with another domain (e.g., machine learning) to create a more unique project idea.\n",
      "2. **Add a twist or challenge**: Incorporate a specific challenge or twist into your project, such as handling anti-scraping measures on Amazon or identifying high-risk users in online payment fraud detection.\n",
      "3. **Explore real-world applications**: Instead of just focusing on the technical aspects of web scraping or AI, explore real-world applications and use cases for your project idea, such as extracting data from e-commerce websites or developing a system to detect online payment fraud.\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 9. Generate Explanation Using Llama3.2 (via Ollama/OpenAI API)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nðŸ§  Generating Llama3.2 similarity analysis...\\n\")\n",
    "\n",
    "completion = client_ai.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an academic evaluator and writing assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": llama_prompt}\n",
    "    ],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "llama_summary = completion.choices[0].message.content.strip()\n",
    "\n",
    "print(llama_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93979693-57a9-46df-a76e-4f166f156d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 10. Use GEMINI for Enhanced Summary (Optional)\n",
    "# =============================================\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "gmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
    "\n",
    "print(\"\\nðŸ§  Generating Gemini summary...\\n\")\n",
    "\n",
    "response = gmodel.generate_content(llama_prompt)\n",
    "gemini_summary = response.text.strip()\n",
    "\n",
    "print(gemini_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9d5e6-3ce7-4a28-b597-5f0bbd439a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
