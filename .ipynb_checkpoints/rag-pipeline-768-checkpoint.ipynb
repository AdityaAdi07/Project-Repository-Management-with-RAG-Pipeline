{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea99448-3de3-4500-833f-d2f2f9dafd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Name: DBMS-25\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Path to your Chroma DB (the folder where embeddings were saved)\n",
    "CHROMA_PATH = \"./chroma_store\"   # <-- change if you used a different path\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "# List all existing collections\n",
    "collections = client.list_collections()\n",
    "\n",
    "for c in collections:\n",
    "    print(\"Collection Name:\", c.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfcc5fc-3789-4fcb-8057-7f1f06b2ccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushm\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbadd123-0a2c-4e61-a089-ab39b75fd8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd17 Using Ollama backend at http://localhost:11434 with model: llama3.2\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "MODEL_BACKEND = os.getenv(\"MODEL_BACKEND\", \"ollama\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"llama3.2\")\n",
    "OLLAMA_API_BASE = os.getenv(\"OLLAMA_API_BASE\", \"http://localhost:11434\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"ollama\")  # default dummy key\n",
    "\n",
    "# --- Configure OpenAI client depending on backend ---\n",
    "if MODEL_BACKEND.lower() == \"ollama\":\n",
    "    # Point the OpenAI client to Ollama\u2019s local API\n",
    "    os.environ[\"OPENAI_API_BASE\"] = f\"{OLLAMA_API_BASE}/v1\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    print(f\"\ud83d\udd17 Using Ollama backend at {OLLAMA_API_BASE} with model: {MODEL_NAME}\")\n",
    "else:\n",
    "    print(f\"\ud83d\udd17 Using cloud backend with model: {MODEL_NAME}\")\n",
    "\n",
    "# --- Initialize client ---\n",
    "client_ai = OpenAI()\n",
    "\n",
    "# --- Connect to ChromaDB ---\n",
    "client = chromadb.PersistentClient(path=\"./chroma_store\")\n",
    "collection = client.get_collection(\"DBMS-25\")\n",
    "\n",
    "# --- Load Sentence Transformer ---\n",
    "embedder = SentenceTransformer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2acc83-fe72-4da5-bfcb-d9b206d0cf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Query Configuration ---\n",
    "query = \"web scrapping amazon deals\"\n",
    "TOP_K = 10  # fetch more, then filter unique\n",
    "\n",
    "query_emb = embedder.encode(query).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91588ac0-6391-45b2-be40-221b06c640b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidDimensionException",
     "evalue": "Embedding dimension 384 does not match collection dimensionality 768",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidDimensionException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Query Chroma ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m res = \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_emb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadatas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdistances\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m ids = res[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m      9\u001b[39m docs = res[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\api\\models\\Collection.py:197\u001b[39m, in \u001b[36mCollection.query\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[32m    159\u001b[39m \n\u001b[32m    160\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    181\u001b[39m (\n\u001b[32m    182\u001b[39m     valid_query_embeddings,\n\u001b[32m    183\u001b[39m     valid_n_results,\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m     include,\n\u001b[32m    195\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m query_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_query_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_n_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_where\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_where_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_query_response(query_results, include)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:146\u001b[39m, in \u001b[36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity < granularity:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\api\\segment.py:677\u001b[39m, in \u001b[36mSegmentAPI._query\u001b[39m\u001b[34m(self, collection_id, query_embeddings, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    675\u001b[39m coll = \u001b[38;5;28mself\u001b[39m._get_collection(collection_id)\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m query_embeddings:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._executor.knn(\n\u001b[32m    680\u001b[39m     KNNPlan(\n\u001b[32m    681\u001b[39m         Scan(coll),\n\u001b[32m   (...)\u001b[39m\u001b[32m    691\u001b[39m     )\n\u001b[32m    692\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\chromadb\\api\\segment.py:751\u001b[39m, in \u001b[36mSegmentAPI._validate_dimension\u001b[39m\u001b[34m(self, collection, dim, update)\u001b[39m\n\u001b[32m    749\u001b[39m         \u001b[38;5;28mself\u001b[39m._sysdb.update_collection(\u001b[38;5;28mid\u001b[39m=\u001b[38;5;28mid\u001b[39m, dimension=dim)\n\u001b[32m    750\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m collection[\u001b[33m\"\u001b[39m\u001b[33mdimension\u001b[39m\u001b[33m\"\u001b[39m] != dim:\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidDimensionException(\n\u001b[32m    752\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match collection dimensionality \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection[\u001b[33m'\u001b[39m\u001b[33mdimension\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    753\u001b[39m     )\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mInvalidDimensionException\u001b[39m: Embedding dimension 384 does not match collection dimensionality 768"
     ]
    }
   ],
   "source": [
    "# --- Query Chroma ---\n",
    "res = collection.query(\n",
    "    query_embeddings=[query_emb],\n",
    "    n_results=TOP_K,\n",
    "    include=[\"metadatas\", \"documents\", \"distances\", \"embeddings\"]\n",
    ")\n",
    "\n",
    "ids = res[\"ids\"][0]\n",
    "docs = res[\"documents\"][0]\n",
    "metas = res[\"metadatas\"][0]\n",
    "stored_embs = res[\"embeddings\"][0]\n",
    "distances = res[\"distances\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa7443d-87ca-417e-8ddf-4b372ebca5c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Utility ---\n",
    "def to_percent(x): return round(float(x) * 100, 2)\n",
    "\n",
    "# --- Compute Similarities ---\n",
    "results = []\n",
    "for i, rid in enumerate(ids):\n",
    "    meta = metas[i] or {}\n",
    "    title = meta.get(\"title\", \"\")\n",
    "    domain = meta.get(\"domain\", \"\")\n",
    "    tech_stack = meta.get(\"tech_stack\", \"\")\n",
    "    source = meta.get(\"source\", \"\")\n",
    "    desc = docs[i] or \"\"\n",
    "    objective = meta.get(\"objective\", \"\")\n",
    "\n",
    "    # embeddings\n",
    "    title_emb = embedder.encode(title).tolist() if title else None\n",
    "    desc_emb = embedder.encode(desc).tolist() if desc else None\n",
    "    tech_emb = embedder.encode(tech_stack).tolist() if tech_stack else None\n",
    "    obj_emb = embedder.encode(objective).tolist() if objective else None\n",
    "\n",
    "    # sims\n",
    "    sim_title = cosine_similarity([query_emb], [title_emb])[0][0] if title_emb else 0\n",
    "    sim_desc = cosine_similarity([query_emb], [desc_emb])[0][0] if desc_emb else 0\n",
    "    sim_tech = cosine_similarity([query_emb], [tech_emb])[0][0] if tech_emb else 0\n",
    "    sim_obj = cosine_similarity([query_emb], [obj_emb])[0][0] if obj_emb else 0\n",
    "    sim_whole = cosine_similarity([query_emb], [stored_embs[i]])[0][0]\n",
    "\n",
    "    results.append({\n",
    "        \"id\": rid,\n",
    "        \"title\": title,\n",
    "        \"domain\": domain,\n",
    "        \"tech_stack\": tech_stack,\n",
    "        \"source\": source,\n",
    "        \"sim_whole\": sim_whole,\n",
    "        \"sim_title\": sim_title,\n",
    "        \"sim_description\": sim_desc,\n",
    "        \"sim_tech_stack\": sim_tech,\n",
    "        \"sim_objective\": sim_obj,\n",
    "        \"doc_snippet\": desc[:300]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a57d46-8058-4efe-b2c7-991fa77183ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== \ud83c\udfaf Unique Top-5 Results After Deduplication ===\n",
      "\n",
      "1. Scraping laptop data from Amazon\n",
      "   Domain: Artificial Intelligence / Data Science\n",
      "   Source: ISE-dept\n",
      "   Tech Stack: Python 3.9, BeautifulSoup4, requests, pandas, NumPy\n",
      "   Whole Similarity: 58.13%\n",
      "   Title Sim: 61.03%, Desc Sim: 58.13%\n",
      "   Snippet: Scraping laptop data from Amazon - This project aims to use Python libraries to scrape and extract data on laptop models, features, and pricing from A...\n",
      "\n",
      "2. Beautiful Soup for Web Scraping\n",
      "   Domain: Artificial Intelligence / Data Science\n",
      "   Source: ISE-dept\n",
      "   Tech Stack: Python, BeautifulSoup, lxml, requests, HTML, XML\n",
      "   Whole Similarity: 34.46%\n",
      "   Title Sim: 40.56%, Desc Sim: 34.46%\n",
      "   Snippet: Beautiful Soup for Web Scraping - Explore the application of using this Python package for extracting data through the parsing of XML documents and HT...\n",
      "\n",
      "3. Building an E-Commerce Clothing Classifier Model with Keras\n",
      "   Domain: Artificial Intelligence / Data Science\n",
      "   Source: ISE-dept\n",
      "   Tech Stack: Keras, TensorFlow, NumPy, Pandas, OpenCV, Python, JavaScript\n",
      "   Whole Similarity: 32.84%\n",
      "   Title Sim: 33.38%, Desc Sim: 32.84%\n",
      "   Snippet: Building an E-Commerce Clothing Classifier Model with Keras - The Building an E-Commerce Clothing Classifier Model with Keras project focuses on image...\n",
      "\n",
      "4. Online Payment Fraud Detection\n",
      "   Domain: Artificial Intelligence / Data Science\n",
      "   Source: ISE-dept\n",
      "   Tech Stack: Python 3.8+, Pandas, NumPy, matplotlib, Scikit-learn, Flask\n",
      "   Whole Similarity: 31.43%\n",
      "   Title Sim: 34.1%, Desc Sim: 31.43%\n",
      "   Snippet: Online Payment Fraud Detection - This online payment fraud detection project aims to help students learn how to build a system that can identify fraud...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Deduplicate same titles & take top-5 ---\n",
    "unique_titles = {}\n",
    "for r in sorted(results, key=lambda x: x[\"sim_whole\"], reverse=True):\n",
    "    if r[\"title\"] not in unique_titles:\n",
    "        unique_titles[r[\"title\"]] = r\n",
    "\n",
    "final_results = list(unique_titles.values())[:5]\n",
    "\n",
    "# --- Print in terminal ---\n",
    "print(\"\\n=== \ud83c\udfaf Unique Top-5 Results After Deduplication ===\\n\")\n",
    "for i, r in enumerate(final_results, start=1):\n",
    "    print(f\"{i}. {r['title']}\")\n",
    "    print(f\"   Domain: {r['domain']}\")\n",
    "    print(f\"   Source: {r['source']}\")\n",
    "    print(f\"   Tech Stack: {r['tech_stack']}\")\n",
    "    print(f\"   Whole Similarity: {round(r['sim_whole']*100, 2)}%\")\n",
    "    print(f\"   Title Sim: {round(r['sim_title']*100, 2)}%, Desc Sim: {round(r['sim_description']*100, 2)}%\")\n",
    "    print(f\"   Snippet: {r['doc_snippet'][:150]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93979693-57a9-46df-a76e-4f166f156d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build Prompt for Llama ---\n",
    "prompt_lines = [\n",
    "    f\"Query: {query}\",\n",
    "    \"The system retrieved these 5 most similar student projects:\",\n",
    "    \"\"\n",
    "]\n",
    "for idx, r in enumerate(final_results, start=1):\n",
    "    prompt_lines.append(f\"{idx}. {r['title']}\")\n",
    "    prompt_lines.append(f\"   - Domain: {r['domain']}\")\n",
    "    prompt_lines.append(f\"   - Similarities: Overall {to_percent(r['sim_whole'])}%, Title {to_percent(r['sim_title'])}%, Desc {to_percent(r['sim_description'])}%, Tech {to_percent(r['sim_tech_stack'])}%\")\n",
    "    prompt_lines.append(f\"   - Snippet: {r['doc_snippet']}\\n\")\n",
    "\n",
    "prompt_lines.append(\n",
    "    \"Now act as a project evaluation assistant. For each project:\\n\"\n",
    "    \"1. Explain briefly WHY it matched this query.\\n\"\n",
    "    \"2. Suggest 2 ways to make the new idea original.\\n\"\n",
    "    \"End with 3 general originality improvement tips.\"\n",
    ")\n",
    "llama_prompt = \"\\n\".join(prompt_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fcd1414-a845-40b8-bb36-86195798a12b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83e\udde0 Generating synthesized explanation\n",
      "\n",
      "\n",
      "=== \ud83d\udd0d Top-5 Results ===\n",
      "\n",
      "1. Scraping laptop data from Amazon\n",
      "   Domain: Artificial Intelligence / Data Science | Whole Sim: 58.13%\n",
      "   Field Sims -> Title: 61.03%, Desc: 58.13%, Tech: 21.47%\n",
      "   Snippet: Scraping laptop data from Amazon - This project aims to use Python libraries to scrape and extract data on laptop models, features, and pricing from A...\n",
      "\n",
      "2. Beautiful Soup for Web Scraping\n",
      "   Domain: Artificial Intelligence / Data Science | Whole Sim: 34.46%\n",
      "   Field Sims -> Title: 40.56%, Desc: 34.46%, Tech: 27.55%\n",
      "   Snippet: Beautiful Soup for Web Scraping - Explore the application of using this Python package for extracting data through the parsing of XML documents and HT...\n",
      "\n",
      "3. Building an E-Commerce Clothing Classifier Model with Keras\n",
      "   Domain: Artificial Intelligence / Data Science | Whole Sim: 32.84%\n",
      "   Field Sims -> Title: 33.38%, Desc: 32.84%, Tech: 7.92%\n",
      "   Snippet: Building an E-Commerce Clothing Classifier Model with Keras - The Building an E-Commerce Clothing Classifier Model with Keras project focuses on image...\n",
      "\n",
      "4. Online Payment Fraud Detection\n",
      "   Domain: Artificial Intelligence / Data Science | Whole Sim: 31.43%\n",
      "   Field Sims -> Title: 34.1%, Desc: 31.43%, Tech: 4.06%\n",
      "   Snippet: Online Payment Fraud Detection - This online payment fraud detection project aims to help students learn how to build a system that can identify fraud...\n",
      "\n",
      "\n",
      "=== \ud83e\udde0 AI Explanation & Suggestions ===\n",
      "\n",
      "I'll evaluate each project based on the query \"web scrapping amazon deals\" and provide suggestions for making the ideas more original.\n",
      "\n",
      "**Project 1: Scraping laptop data from Amazon**\n",
      "\n",
      "Why it matched the query: This project involves web scraping, a technique used to extract data from websites like Amazon. The focus on Amazon deals suggests that the project aims to collect data on product offers, discounts, or promotions.\n",
      "\n",
      "Suggestions for making it more original:\n",
      "\n",
      "1. **Integrate AI-powered analysis**: Instead of just scraping data, use machine learning algorithms to analyze the scraped data and provide insights on trends, patterns, or predictions about future deals.\n",
      "2. **Focus on specific Amazon deal types**: Instead of general laptop data, focus on a specific type of Amazon deal, such as \"Deal of the Day\" or \"Lightning Deals,\" to make the project more focused and unique.\n",
      "\n",
      "**Project 2: Beautiful Soup for Web Scraping**\n",
      "\n",
      "Why it matched the query: While this project doesn't specifically focus on Amazon deals, it does cover web scraping techniques that can be applied to any website, including Amazon. The relevance lies in its applicability to web scraping tasks.\n",
      "\n",
      "Suggestions for making it more original:\n",
      "\n",
      "1. **Integrate with other web scraping tools**: Combine Beautiful Soup with other popular web scraping libraries or frameworks (e.g., Scrapy) to create a more comprehensive and efficient scraper.\n",
      "2. **Explore advanced scraping techniques**: Delve into more complex web scraping topics, such as handling anti-scraping measures, dealing with dynamic content, or using AI-powered web scraping tools.\n",
      "\n",
      "**Project 3: Building an E-Commerce Clothing Classifier Model with Keras**\n",
      "\n",
      "Why it matched the query: Although this project focuses on image classification for e-commerce clothing, it doesn't specifically involve web scraping. However, the context of e-commerce and online shopping is relevant to Amazon deals.\n",
      "\n",
      "Suggestions for making it more original:\n",
      "\n",
      "1. **Integrate web scraping data into the model**: Use scraped data from Amazon or other e-commerce platforms as input for the image classification model, allowing the project to bridge the gap between web scraping and computer vision.\n",
      "2. **Explore alternative classification methods**: Move beyond traditional image classification techniques and explore more advanced approaches, such as object detection, segmentation, or sentiment analysis.\n",
      "\n",
      "**Project 4: Online Payment Fraud Detection**\n",
      "\n",
      "Why it matched the query: This project involves machine learning and data analysis, which can be applied to detecting fraudulent online transactions on Amazon deals.\n",
      "\n",
      "Suggestions for making it more original:\n",
      "\n",
      "1. **Integrate web scraping data into the model**: Use scraped data from Amazon or other e-commerce platforms as input for the fraud detection model, allowing the project to incorporate real-time deal data.\n",
      "2. **Explore anomaly detection techniques**: Focus on detecting unusual patterns in online transactions rather than just classifying them as fraudulent.\n",
      "\n",
      "**General Originality Improvement Tips**\n",
      "\n",
      "1. **Combine multiple domains**: Integrate concepts from multiple domains (e.g., web scraping + machine learning + computer vision) to create a more comprehensive and unique project.\n",
      "2. **Focus on real-world applications**: Make sure the project has practical relevance to Amazon deals or e-commerce in general, rather than just exploring theoretical aspects of web scraping or data analysis.\n",
      "3. **Explore new tools and techniques**: Stay up-to-date with the latest developments in web scraping, machine learning, and other relevant technologies to ensure your project is innovative and cutting-edge.\n"
     ]
    }
   ],
   "source": [
    "# --- Generate Summary with Llama3.2 ---\n",
    "print(\"\\n\ud83e\udde0 Generating synthesized explanation\")\n",
    "completion = client_ai.chat.completions.create(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an academic evaluator and writing assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": llama_prompt}\n",
    "    ],\n",
    "    temperature=0.5\n",
    ")\n",
    "ai_summary = completion.choices[0].message.content.strip()\n",
    "# --- Display Results ---\n",
    "print(\"\\n\\n=== \ud83d\udd0d Top-5 Results ===\")\n",
    "for i, r in enumerate(final_results, start=1):\n",
    "    print(f\"\\n{i}. {r['title']}\")\n",
    "    print(f\"   Domain: {r['domain']} | Whole Sim: {to_percent(r['sim_whole'])}%\")\n",
    "    print(f\"   Field Sims -> Title: {to_percent(r['sim_title'])}%, Desc: {to_percent(r['sim_description'])}%, Tech: {to_percent(r['sim_tech_stack'])}%\")\n",
    "    print(f\"   Snippet: {r['doc_snippet'][:150]}...\")\n",
    "\n",
    "print(\"\\n\\n=== \ud83e\udde0 AI Explanation & Suggestions ===\\n\")\n",
    "print(ai_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed36d05-b953-4123-9c3b-36a9921df879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save JSON Report ---\n",
    "os.makedirs(\"analysis_reports\", exist_ok=True)\n",
    "file_path = f\"analysis_reports/{query.replace(' ', '_')}.json\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"query\": query,\n",
    "        \"results\": final_results,\n",
    "        \"ai_analysis\": ai_summary\n",
    "    }, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n\u2705 Analysis complete and saved to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ce91c-8083-4fd9-899a-eb44401f848e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# --- Load env variables ---\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"gemini-2.5-pro\")\n",
    "\n",
    "# --- Configure Gemini ---\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel(MODEL_NAME)\n",
    "\n",
    "# --- Your prompt (already prepared earlier) ---\n",
    "print(\"\\n\ud83e\udde0 Generating synthesized explanation...\")\n",
    "\n",
    "response = model.generate_content(llama_prompt)   # reuse same prompt structure\n",
    "ai_summary = response.text.strip()\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n\\n=== \ud83d\udd0d Top-5 Results ===\")\n",
    "for i, r in enumerate(final_results, start=1):\n",
    "    print(f\"\\n{i}. {r['title']}\")\n",
    "    print(f\"   Domain: {r['domain']} | Whole Sim: {to_percent(r['sim_whole'])}%\")\n",
    "    print(f\"   Field Sims \u2192 Title: {to_percent(r['sim_title'])}%, \"\n",
    "          f\"Desc: {to_percent(r['sim_description'])}%, \"\n",
    "          f\"Tech: {to_percent(r['sim_tech_stack'])}%\")\n",
    "    print(f\"   Snippet: {r['doc_snippet'][:150]}...\")\n",
    "\n",
    "print(\"\\n\\n=== \ud83e\udde0 AI Explanation & Suggestions  ===\\n\")\n",
    "print(ai_summary)\n",
    "\n",
    "# --- Optional: Save to JSON ---\n",
    "import json\n",
    "out_path = f\"analysis_reports/{query.replace(' ','_')}_gemini.json\"\n",
    "os.makedirs(\"analysis_reports\", exist_ok=True)\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"query\": query,\n",
    "        \"results\": final_results,\n",
    "        \"ai_analysis\": ai_summary\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n\u2705 Saved Gemini analysis to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d706b-e65f-4b65-a150-78b9ae646fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}